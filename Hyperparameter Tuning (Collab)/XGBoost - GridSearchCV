{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPO6lyXaNe+OMxL9h0Q35K3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Pab04QRCrMKf","executionInfo":{"status":"error","timestamp":1741553490081,"user_tz":300,"elapsed":2993,"user":{"displayName":"Andrew Castillo","userId":"02822267340440655112"}},"outputId":"99ada922-33c3-45e8-f7fd-c121ee5f60fa","colab":{"base_uri":"https://localhost:8080/","height":373}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data from cleaned_hospital_readmission.parquet...\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'cleaned_hospital_readmission.parquet'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-0ea353110416>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0mparquet_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cleaned_hospital_readmission.parquet\"\u001b[0m  \u001b[0;31m# Replace with your parquet file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_xgboost_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-0ea353110416>\u001b[0m in \u001b[0;36mrun_xgboost_tuning\u001b[0;34m(parquet_path)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;31m# Load data from parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading data from {parquet_path}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;31m# Convert categorical columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mto_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split_blocks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    268\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mfilesystem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         handles = get_handle(\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cleaned_hospital_readmission.parquet'"]}],"source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import fbeta_score\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from imblearn.over_sampling import SMOTENC\n","from xgboost import XGBClassifier\n","import time\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Feature engineering function\n","def feature_engineering(X):\n","    med_columns = [\n","        'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide',\n","        'glimepiride', 'acetohexamide', 'glipizide', 'glyburide',\n","        'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n","        'miglitol', 'troglitazone', 'tolazamide', 'insulin',\n","        'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone',\n","        'metformin-pioglitazone'\n","    ]\n","\n","    data = X.copy()\n","\n","    # Medication features\n","    data['med_count'] = data[med_columns].apply(lambda x: (x != 'No').sum(), axis=1)\n","    data['on_multiple_meds'] = (data['med_count'] >= 3).astype(int)\n","    data['med_changes_total'] = data[med_columns].apply(\n","        lambda x: ((x == 'Up') | (x == 'Down')).sum(), axis=1)\n","    data['insulin_binary'] = (data['insulin'] != 'No').astype(int)\n","\n","    # Hospital utilization features\n","    data['total_visits'] = data['number_outpatient'] + data['number_emergency'] + data['number_inpatient']\n","    data['high_utilizer'] = ((data['number_inpatient'] > 1) |\n","                            (data['number_emergency'] > 2) |\n","                            (data['number_outpatient'] > 5)).astype(int)\n","\n","    # Admission features\n","    data['is_emergency'] = (data['admission_type'] == 'Urgent/Emergency').astype(int)\n","\n","    return data\n","\n","# Create SMOTE-balanced version of data\n","def apply_smote(X_train, y_train, cat_columns):\n","    cat_indices = [X_train.columns.get_loc(col) for col in cat_columns]\n","    smote = SMOTENC(sampling_strategy=0.5, random_state=42, categorical_features=cat_indices)\n","    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n","    return X_train_balanced, y_train_balanced\n","\n","# F2 score for evaluation\n","def f2_score(y_true, y_pred):\n","    return fbeta_score(y_true, y_pred, beta=2)\n","\n","# Function to tune XGBoost with minimal compute\n","def tune_xgboost_efficient(X_train, X_val, y_train, y_val, cat_columns, numerical_columns):\n","    results = {}\n","\n","    # Minimal parameter grid for efficiency\n","    param_grid = {\n","        'n_estimators': [100, 200],\n","        'learning_rate': [0.05, 0.1],\n","        'max_depth': [3, 5],\n","        'subsample': [0.8, 1.0]\n","    }\n","\n","    # Preprocessor for XGBoost\n","    preprocessor = ColumnTransformer(\n","        transformers=[\n","            ('cat', OneHotEncoder(handle_unknown='ignore'), cat_columns),\n","            ('num', StandardScaler(), numerical_columns)\n","        ])\n","\n","    # 1. Tune XGBoost with SMOTE\n","    print(\"Tuning XGBoost with SMOTE (Efficient Approach)...\")\n","    X_train_balanced, y_train_balanced = apply_smote(X_train, y_train, cat_columns)\n","\n","    # Pre-process validation data once to save time\n","    preprocessor_val = clone(preprocessor)\n","    X_val_processed = preprocessor_val.fit_transform(X_val)\n","\n","    best_score_smote = -float('inf')\n","    best_params_smote = None\n","\n","    # Create a dataframe to store all results\n","    smote_results = []\n","\n","    # Manual grid search for SMOTE approach\n","    for n_estimators in param_grid['n_estimators']:\n","        for learning_rate in param_grid['learning_rate']:\n","            for max_depth in param_grid['max_depth']:\n","                for subsample in param_grid['subsample']:\n","                    # Create and preprocess training data\n","                    preprocessor_train = clone(preprocessor)\n","                    X_train_processed = preprocessor_train.fit_transform(X_train_balanced)\n","\n","                    # Set parameters\n","                    params = {\n","                        'n_estimators': n_estimators,\n","                        'learning_rate': learning_rate,\n","                        'max_depth': max_depth,\n","                        'subsample': subsample,\n","                        'colsample_bytree': 0.8,  # Fixed\n","                        'random_state': 1984,\n","                        'verbosity': 0\n","                    }\n","\n","                    # Train the model\n","                    model = XGBClassifier(**params)\n","                    model.fit(\n","                        X_train_processed, y_train_balanced,\n","                        early_stopping_rounds=50,\n","                        eval_set=[(X_val_processed, y_val)],\n","                        verbose=False\n","                    )\n","\n","                    # Evaluate on validation set\n","                    y_pred = model.predict(X_val_processed)\n","                    score = f2_score(y_val, y_pred)\n","\n","                    smote_results.append({\n","                        'n_estimators': n_estimators,\n","                        'learning_rate': learning_rate,\n","                        'max_depth': max_depth,\n","                        'subsample': subsample,\n","                        'f2_score': score\n","                    })\n","\n","                    print(f\"SMOTE - N Estimators: {n_estimators}, Learning Rate: {learning_rate}, \"\n","                          f\"Max Depth: {max_depth}, Subsample: {subsample}, F2: {score:.4f}\")\n","\n","                    if score > best_score_smote:\n","                        best_score_smote = score\n","                        best_params_smote = params.copy()\n","\n","    # 2. Tune XGBoost with class weights\n","    print(\"\\nTuning XGBoost with class weights (Efficient Approach)...\")\n","    # Calculate class weight\n","    neg_count, pos_count = np.bincount(y_train)\n","    scale_pos_weight = neg_count / pos_count\n","\n","    # Pre-process the original training data for class weights\n","    preprocessor_train_orig = clone(preprocessor)\n","    X_train_processed_orig = preprocessor_train_orig.fit_transform(X_train)\n","\n","    best_score_cw = -float('inf')\n","    best_params_cw = None\n","\n","    # Create a dataframe to store all results\n","    cw_results = []\n","\n","    # Manual grid search for class weights approach\n","    for n_estimators in param_grid['n_estimators']:\n","        for learning_rate in param_grid['learning_rate']:\n","            for max_depth in param_grid['max_depth']:\n","                for subsample in param_grid['subsample']:\n","                    # Set parameters with scale_pos_weight\n","                    params = {\n","                        'n_estimators': n_estimators,\n","                        'learning_rate': learning_rate,\n","                        'max_depth': max_depth,\n","                        'subsample': subsample,\n","                        'colsample_bytree': 0.8,  # Fixed\n","                        'scale_pos_weight': scale_pos_weight,\n","                        'random_state': 1984,\n","                        'verbosity': 0\n","                    }\n","\n","                    # Train the model\n","                    model = XGBClassifier(**params)\n","                    model.fit(\n","                        X_train_processed_orig, y_train,\n","                        early_stopping_rounds=50,\n","                        eval_set=[(X_val_processed, y_val)],\n","                        verbose=False\n","                    )\n","\n","                    # Evaluate on validation set\n","                    y_pred = model.predict(X_val_processed)\n","                    score = f2_score(y_val, y_pred)\n","\n","                    cw_results.append({\n","                        'n_estimators': n_estimators,\n","                        'learning_rate': learning_rate,\n","                        'max_depth': max_depth,\n","                        'subsample': subsample,\n","                        'f2_score': score\n","                    })\n","\n","                    print(f\"Class Weights - N Estimators: {n_estimators}, Learning Rate: {learning_rate}, \"\n","                          f\"Max Depth: {max_depth}, Subsample: {subsample}, F2: {score:.4f}\")\n","\n","                    if score > best_score_cw:\n","                        best_score_cw = score\n","                        best_params_cw = params.copy()\n","\n","    # Create full dataframes of results\n","    smote_results_df = pd.DataFrame(smote_results)\n","    cw_results_df = pd.DataFrame(cw_results)\n","\n","    results = {\n","        'SMOTE': {\n","            'best_params': best_params_smote,\n","            'best_score': best_score_smote,\n","            'all_results': smote_results_df\n","        },\n","        'Class Weights': {\n","            'best_params': best_params_cw,\n","            'best_score': best_score_cw,\n","            'all_results': cw_results_df\n","        }\n","    }\n","\n","    return results, preprocessor_val\n","\n","# Main function to run XGBoost hyperparameter tuning\n","def run_xgboost_tuning(parquet_path):\n","    start_time = time.time()\n","\n","    # Load data from parquet\n","    print(f\"Loading data from {parquet_path}...\")\n","    df = pd.read_parquet(parquet_path)\n","\n","    # Convert categorical columns\n","    categorical_cols = [col for col in df.columns if df[col].dtype == 'object']\n","    for col in categorical_cols:\n","        df[col] = df[col].astype('category')\n","\n","    # Convert target to binary\n","    df[\"readmitted_binary\"] = df[\"readmitted\"].apply(lambda x: 1 if x == \"<30\" else 0)\n","\n","    # Split data\n","    X = df.drop(columns=[\"readmitted\", \"readmitted_binary\"])\n","    y = df[\"readmitted_binary\"]\n","\n","    # Apply feature engineering\n","    X_enriched = feature_engineering(X)\n","\n","    # Identify categorical features\n","    cat_columns = X_enriched.select_dtypes(include=['object', 'category']).columns.tolist()\n","    numerical_columns = X_enriched.select_dtypes(include=['int64', 'float64']).columns.tolist()\n","\n","    # Split data into train and validation sets\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        X_enriched, y, random_state=1984, stratify=y, test_size=0.2\n","    )\n","\n","    # Tune XGBoost efficiently\n","    print(\"\\n=== Efficient XGBoost Tuning ===\")\n","    results, preprocessor = tune_xgboost_efficient(X_train, X_val, y_train, y_val, cat_columns, numerical_columns)\n","\n","    # Print results summary\n","    print(\"\\n=== RESULTS SUMMARY ===\")\n","    print(\"\\nSMOTE - Best F2 Score:\", results['SMOTE']['best_score'])\n","    print(\"SMOTE - Best Parameters:\")\n","    for param, value in results['SMOTE']['best_params'].items():\n","        if param not in ['random_state', 'verbosity']:\n","            print(f\"  {param}: {value}\")\n","\n","    print(\"\\nClass Weights - Best F2 Score:\", results['Class Weights']['best_score'])\n","    print(\"Class Weights - Best Parameters:\")\n","    for param, value in results['Class Weights']['best_params'].items():\n","        if param not in ['random_state', 'verbosity']:\n","            print(f\"  {param}: {value}\")\n","\n","    # Save results to CSV\n","    results['SMOTE']['all_results'].to_csv('xgboost_smote_tuning_results.csv', index=False)\n","    results['Class Weights']['all_results'].to_csv('xgboost_cw_tuning_results.csv', index=False)\n","\n","    # Train final models with best params for feature importance analysis\n","    print(\"\\n=== Training Final Models for Feature Importance ===\")\n","\n","    # SMOTE best model\n","    X_train_balanced, y_train_balanced = apply_smote(X_train, y_train, cat_columns)\n","    X_train_processed = preprocessor.transform(X_train_balanced)\n","\n","    best_model_smote = XGBClassifier(**results['SMOTE']['best_params'])\n","    best_model_smote.fit(X_train_processed, y_train_balanced)\n","\n","    # Class weights best model\n","    X_train_processed_orig = preprocessor.transform(X_train)\n","\n","    best_model_cw = XGBClassifier(**results['Class Weights']['best_params'])\n","    best_model_cw.fit(X_train_processed_orig, y_train)\n","\n","    # Create feature names after one-hot encoding\n","    feature_names = []\n","    for name, encoder, cols in preprocessor.transformers_:\n","        if name == 'cat':\n","            for col in cols:\n","                # Get categories from OneHotEncoder\n","                categories = encoder.categories_[cols.index(col)]\n","                for cat in categories:\n","                    feature_names.append(f\"{col}_{cat}\")\n","        else:\n","            feature_names.extend(cols)\n","\n","    # Get feature importance\n","    smote_importance = pd.DataFrame({\n","        'Feature': feature_names[:len(best_model_smote.feature_importances_)],\n","        'Importance': best_model_smote.feature_importances_\n","    }).sort_values('Importance', ascending=False).head(20)\n","\n","    cw_importance = pd.DataFrame({\n","        'Feature': feature_names[:len(best_model_cw.feature_importances_)],\n","        'Importance': best_model_cw.feature_importances_\n","    }).sort_values('Importance', ascending=False).head(20)\n","\n","    print(\"\\nTop features (SMOTE):\")\n","    print(smote_importance)\n","\n","    print(\"\\nTop features (Class Weights):\")\n","    print(cw_importance)\n","\n","    # Save feature importance\n","    smote_importance.to_csv('xgboost_smote_feature_importance.csv', index=False)\n","    cw_importance.to_csv('xgboost_cw_feature_importance.csv', index=False)\n","\n","    # Print execution time\n","    execution_time = time.time() - start_time\n","    print(f\"\\nTotal execution time: {execution_time/60:.2f} minutes\")\n","\n","    return results\n","\n","# Run the tuning if this script is executed\n","if __name__ == \"__main__\":\n","    parquet_path = \"cleaned_hospital_readmission.parquet\"  # Replace with your parquet file path\n","    results = run_xgboost_tuning(parquet_path)"]},{"cell_type":"code","source":[],"metadata":{"id":"cknX8A9crOQN"},"execution_count":null,"outputs":[]}]}